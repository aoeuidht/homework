\section{chapter 3}
\subsection*{3.A}
\subsubsection*{3.A.1}

If $T$ is linear, make $u=(x, y, z)$ and $v=2u$, then we should have $Tv = 2Tu$
\[(4x-8y+6z+b, 12x+c\cdot 8xyz) = 2\cdot (2x-4y+3z+b, 6x+cxyz)\]

then we have $b=c=0$

If $b=c=0$

\begin{equation*}
    \begin{split}
T(u+v) &= (2(x_{u}+x_{v}) - 4(y_{u}+y_{v}) + 3(z_{u}+z_{v}), 6(x_{u}+x_{v}) \\
&= (2x_{u}-4y_{u}+3z_{u}, 6x_{u}) + (2x_{v}-4y_{v}+3z_{v}, 6x_{v}) \\
&= Tu+Tv \\
T(\lambda u) &=(2\lambda x - 4\lambda y + 3\lambda z, 6\lambda x) \\
&= \lambda (2x-4y+3z, 6x) \\
&= \lambda Tu
\end{split}
\end{equation*}

\subsubsection*{3.A.2}

If $T$ is linear, then for $p_{0}, p_{1}$, $T(p_{0}+p_{1}) = Tp_{0} + Tp_{1}$, we must have 
\[b(p_{0}(1)+p_{1}(1))(p_{0}(2)+p_{1}(2)) = b(p_{0}(1)p_{0}(2)+p_{1}(1)p_{1}(2))\]

Expand the left part, to make the equation work, we have $b=0$

For any $\lambda \in R$,
\[c\lambda \sin{p(0)} = c \sin{\lambda p{0}}\]

make $\lambda = 2$, to make the equation work, $c=0$.

If $b=c=0$, we can see that the $T$ is the linear combination by linear maps in $3.4$, so $T$ is also linear.

\subsubsection*{3.A.4}

If $v_{1}, \cdots , v_{m}$ is dependent, then $ \exists a_{1}, \cdots, a_{m} $ that 
\begin{equation*}
    \begin{split}
0 &= a_{1}v_{1} + \cdots + a_{m}v_{m} \\
0 &= T(0) \\
 &= T(a_{1}v_{1} + \cdots + a_{m}v_{m}) \\
    & = Ta_{1}v_{1} + \cdots + Ta_{m}v_{m} \\
    & = a_{1}Tv_{1} + \cdots + a_{m}Tv_{m}
\end{split}
\end{equation*}

Which is a counter example of $Tv_{1}, \cdots, Tv_{m}$ is independent.

\subsubsection*{3.A.7}

If $dim(V) = 1$, let's assume $v$ is a basis of $V$, based on the definition of span, then we have 
\[V=\{av\, a\in F\}\]

IF $T\in L(V,V)$, then we have $Tv\in V$, so $Tv = \lambda v$.

\subsubsection*{3.A.8}

\[\varphi(a_{0}, a_{1}) = \sqrt{a_{0}^{2} + a_{1}^{2}}\]

\subsubsection*{3.A.9}
\[\varphi(i, j) = (i, 0)\]

\subsubsection*{3.A.10}

$T$ is not linear, because it does not fulfill $T(u+v) = Tu + Tv$:

\[u\in U, v \in (V - U)\] 

based on the defination, we have 

\begin{equation*}
    \begin{split}
u+v &\in (V-U) \\
Tu &\neq 0  \\
Tv &= 0 \\
T(u+v) &= 0 \\
T(u+v) &\neq Tu + tv
\end{split}
\end{equation*}

So $T$ is not linear.

\subsubsection*{3.A.11}

$U$ is a subspace of $V$, so we can find a basis of $U$: $u_{1}, \cdots, u_{n}$, and we can expand the basis of $U$ to basis of $W$ to $u_{1}, \cdots, u_{n}, q_{1}, \cdots, q_{m} $.

Make 
\[Q = span(q_{1}, \cdots, q_{m})\]

and $R\in L(Q, W)$.

Define the $T$ like:

\[Q = span(q_{1}, \cdots, q_{m})\]

 \[
    Tv=\left\{
 \begin{array}{ll}
   Sv    &    v\in U \\
   Rv    &    v \in Q \\
   Sv_{0} + Rv_{1}    &    v = v_{0} + v_{1}, v_{0} \in U, v_{1} \in Q
  \end{array}
  \right.
  \]

\subsubsection*{3.A.12}

If $v_{1},\cdots, v_{n}$ is basis of $V$, and $S_{1}, \cdots, S_{m}$ is basis of $L(V, W)$, then we have 
\[W=span(S_{1}v_{1}, \cdots, S_{1}v_{n}, \cdots, S_{m}v_{1},\cdots, S_{m}v_{n})\]
which is conflict with that $W$ is infinite dimensional.

\subsubsection*{3.A.13}

$v_{1},\cdots,v_{m}$ is linearly dependent, so $\exists v_{i}$ that satisfy:
\[v_{i} = a_{1}v_{1} + \cdots + a_{i-1}v_{i-1} + a_{i+1}v_{i+1} + \cdots + a_{m}v_{m}\]

so if we use any $T\in L(v, W)$, the $T^{'}$ to convert $v_{1}, \cdots, v_{m}$ to following vectors does not exists for any  $w\in W,w\neq 0$:

\[Tv_{1}, \cdots, Tv_{i-1}, w + (aTv_{1}  + \cdots + a_{i-1}Tv_{i-1} + a_{i+1}Tv_{i+1} + \cdots + a_{m}Tv_{m}), Tv_{i+1}, \cdots, Tv_{m}\]


\subsubsection*{3.A.14}
 
Here is a counter example:

\begin{equation*}
    \begin{split}
T(x_{0}, \cdots, x_{n}) &= (x_{0}+x_{1}, x_{1}, \cdots, x_{n}) \\
S(x_{0}, \cdots, x_{n}) &= (x_{0}, x_{0}+ x_{1}, \cdots, x_{n})
\end{split}
\end{equation*}

\subsection*{3.B}
\subsubsection*{3.B.1}

\[T(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}) = (x_{4}, x_{5})\]

\subsubsection*{3.B.2}
\[(ST)^{2}(v) = S(T(ST(v)))\]
Since range $S\subset null T$, so we have $T(ST(v)) = 0$, implies $(ST)^{2} = 0$.

\subsubsection*{3.B.3}

\begin{enumerate}[label=(\alph*)]
\item $range T = V$
\item $null T = 0$
\end{enumerate}


\subsubsection*{3.B.4}

We have an count example:

\begin{equation*}
    \begin{split}
T_{1}(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}) &= (x_{1}, 0, 0, 0) \\
T_{2}(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}) &= (0, x_{2}, 0, 0) \\
T_{3}(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}) &= (0, 0, x_{3}, 0) \\
T_{4}(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}) &= (0, 0, 0, x_{4}) \\
T_{1} + T_{2} + T_{3} + T_{4} &= (x_{1}, x_{2}, x_{3}, x_{4})
\end{split}
\end{equation*}

The last $null T = (5-5) = 1$, so it's not the subspace.

\subsubsection*{3.B.6}

$dim(T) = range T + null T, range T = null T \implies null T = 2.5$, which make no sense.

\subsubsection*{3.B.9}

If $Tv_{1}, \cdots, Tv_{n}$ is dependent, then we have $a_{1}, \cdots, a_{n}$ that:
\[a_{1}Tv_{1} + \cdots +a_{n}Tv_{n} = T(a_{1}v_{1}+\cdots+a_{n}v_{n}) = 0\]

Since T is injective, and $T(0) = 0$, implies $a_{1}v_{1}+\cdots+a_{n}v_{n} = 0$, which is a counter example of they are linearly independent.

\subsubsection*{3.B.10}

For $\forall w\in range T$, there exists a $v\in V, Tv=w$. Since $v_{1}, \cdots, v_{n}$ spans $V$, we have 
\[v = a_{1}v_{1} + \cdots + a_{n}v_{n}\]

So 

\[w = Tv = Ta_{1}v_{1} + \cdots + Ta_{n}v_{n} = a_{1}Tv_{1} + \cdots + a_{n}Tv_{n}\]

indicates that $Tv_{1}, \cdots, Tv_{n}$ spans range $T$.

\subsubsection*{3.B.11}


\begin{equation*}
    \begin{split}
S_{1}\cdots S_{n}(u) &= S_{1}\cdots S_{n}(v) \implies \\
S_{2}\cdots S_{n}(u) &= S_{2}\cdots S_{n}(v) \implies \\
&\cdots \\
S_{n}(u) &= S_{n}(v) \implies \\
u &= v
\end{split}
\end{equation*}

\subsubsection*{3.B.12}

Make $u_{1}, \cdots, u_{n}$ a basis of $null T$, then we can expand it to $u_{1}, \cdots, u_{n}, w_{1}, \cdots, w_{m}$ to the basis of $V$.

$U=span(w_{1},\cdots, w_{m}$ is the subspace we need.

\subsubsection*{3.B.13}

$dim(null T) = 2$, so $dim(range T) = 4 - 2 = 2$, so T is surjective.

\subsubsection*{3.B.15}

the target space has dimension $2$, and $dim(F^{2}) \leq 2$, so $range(null T) + dim(T) \leq 4$ which is less than 5, so the transaction does exists.

\subsubsection*{3.B.16}

Since $null T, range T$ are finite dimensional, assume $u_{1}, \cdots, u_{n}$ is a basis of $null T$ and $v_{1},\cdots, v_{m}$ is a basis of $range T$, then there is a vector $w_{1},\cdots,w_{m}$ defined by
\[Tw_{1} = v_{1}, \cdots, Tw_{m} = v_{m}\]

then we have $u_{1}, \cdots, u_{n}, w_{1}, \cdots, w_{m}$ is a basis of $V$. 

If not, there $\exists v_{0}$ that $u_{1}, \cdots, u_{n}, w_{1}, \cdots, w_{m}, v_{0}$ is linear independent, which indicate that
\[ Tu_{1}, \cdots, Tu_{n}, Tw_{1}, \cdots, Tw_{m}, Tv_{0} = 0, \cdots, 0, v_{1}, \cdots, v_{m}, Tv_{0}\] 

is independent, which is conflict that $v_{1},\cdots, v_{m}$ is a basis of $range T$.

\subsubsection*{3.B.17}

$dim\, V = n, dim\, W = m$, If $n \leq m$, then length of basis $W=w_{1},\cdots,w_{m}$ is longer than basis $V=v_{1},\cdots,v_{n}$, then we can define a injective map as:
\[Tv_{1} = w_{1}, \cdots, Tv_{n} = w_{n}\]

When there exists an injective map, we have $dim\,V\leq dim\,W$ using $3.23$.

\subsubsection*{3.B.18}

if $dim\,V\geq dim\,W$, then any basis of  $V=v_{1},\cdots,v_{n}$ is longer than the basis of $W=w_{1}, \cdots,w_{m}$ ($n\geq m)$. So here in a surjective map:

\begin{equation*}
    \begin{split}
Tv_{1} & = w_{1} \\
& \cdots  \\
Tv_{m} &= w_{m} \\
tv_{m+1} &= 0 \\
&\cdots \\
tv_{n} &= 0
\end{split}
\end{equation*}

If there is a surjective map, and
\[dim\,V = dim(null\,T) + dim(T) = dim(null\, T) + dim\,W\]
which implies $dim\,V\geq dim\,W$.

\subsubsection*{3.B.19}

If $null\,T=U$, we have $dim(range\,T) = dim\,V - dim\,U \leq dim\,W$, the $=$ works only T is surjective.
since $U$ is a subspace, we can write a basis of $V$ as:
\[u_{1}, \cdots, u_{n}, v_{1},\cdots, v_{m}\]

If $W$ has lower dimension $x\leq dim\,V-dim\,U$, then we can build a map that make $null\,T=U$
\begin{equation*}
    \begin{split}
    Tu_{1} &= 0 \\
    & \cdots \\
    Tu_{n} &= 0 \\
Tv_{1} & = w_{1} \\
& \cdots  \\
Tv_{x} &= w_{x} \\
tv_{x+1} &= w_{1} \\
&\cdots \\
tv_{m} &= w_{1}
\end{split}
\end{equation*}

\subsubsection*{3.B.20}

If $T$ is injective, then define $S$ as $S(Tv) = v:v\in V$, so $STv = S(Tv) = v = Iv \implies ST$ is the identity map.

If $T$ is not a one-one map, then $\exists\, Tv_{1} = Tv_{2} = s$, and $Ss$ can't be both $v_{1}$ and $v_{2}$, so no $ST = I$ exists.

\subsubsection*{3.B.21}

IF $ST = I$ and $T$ is not surjective, then $\exists\,w \in W, w\notin range\,T$, so $T(Sw)\notin W$, which is a counter example of $ST = I$.

If T is surjective, then $\forall\,w \in W, \exists\,v\in V, Tv = w$, so let's define $S(Tv) = v: Tv\in W$, and we have $ST = I$.

\subsubsection*{3.B.22}

If dim null (range $T$) = dim null $S$, then 
\[dim\, null\, ST = dim\, null\, S + dim\,null\,T\]

else 

\[dim\, null\, ST < dim\, null\, S + dim\,null\,T\]

\subsubsection*{3.B.23}

Because $T(U) \subseteq V$, so $range\,ST\subseteq range\,S$, so 
\[dim\,range\,ST\leq dim\,range\,S\]

$X = range\, T$ is also a vector spaces, and we $S\in L(X, w)$, we have 
\[dim X = dim\, null\,S + dim\,range S\]

so 
\[dim\,range S \leq dim\,X\]
\[dim\,range ST \leq dim\,range T\]

finally, we have
\[dim\,range\,ST\leq min\{dim\,range\,S,\, dim\,range\,T\}\]

\subsubsection*{3.B.24}

If $T_{2} = ST_{1}$, and $v_{1},\cdots,v_{n}$ is a basis of $null\, T_{1}$, then we have $\forall i \in [1,\cdots,n] T_{2}v_{i}=0$ because $T(0) = 0$, so
    null $T_{1}\subset$ null $ T_{2}$.
    
If null $T_{1} \subset$ null $T_{2}$, suppose $u_{1},\cdots,u_{n}$ is a basis of null $T_{1}$,  $w_{1}, \cdots,w_{m}$ is a basis of null $T_{2} (n\leq m)$, extend both basis of null space to the basis of $V$, we have 2 basis:
\[u_{1},\cdots,u_{n},\cdots, u_{v}\]
\[w_{1},\cdots,w_{n},\cdots,w_{m}, \cdots, w_{v}\]

It's easy to prove(2.39) that $u_{n+1},\cdots,u_{v}$ is the basis of range $T_{1}$ and $w_{m+1},\cdots,w_{v}$ is the basis of range $T_{2}$, so we build a mapping $S$ that

\begin{equation*}
    \begin{split}
    ST_{1}u_{n+1} &= 0 \\
    &\cdots \\
    ST_{1}u_{m} &= 0 \\
    ST_{1}u_{m+1} &= T_{2}w_{m+1} \\
    &\cdots \\
    ST_{1}u_{v} &= T_{2}w_{v}
\end{split}
\end{equation*}

\subsubsection*{3.B.25}

If $\exists S\in L(V, V)$ and $T_{1} = T_{2}S$, then we have range $S \subset V$, so $T_{2}(S) \subset T_{2}(V)$, aka range $T_{1} \subset$ range $T_{2}$.

If range $T_{1}\subset $range $T_{2} \subset W$, and $w_{1}, \cdots, w_{n}$ is a basis of range $T_{1}$. 
This basis can be extend to the basis of range $T_{2}$ like $w_{1}, \cdots, w_{n}, \cdots, w_{m}$, and finally to the basis of $W$: $w_{1},\cdots,w_{m},\cdots, w_{w}$.

For $T_{1}$, we have:
\begin{equation*}
    \begin{split}
T_{1}u_{1} &= w_{1} \\
&\cdots \\
T_{1}u_{n} &= w_{n} \\
T_{1}u_{n+1} &= 0 \\
&\cdots \\
T_{1}u_{v} &= 0 
\end{split}
\end{equation*}

In the equation, $v$ is the dimension of $V$. and for $T_{2}$:
\begin{equation*}
    \begin{split}
T_{2}v_{1} &= w_{1} \\
&\cdots \\
T_{2}v_{m} &= w_{m} \\
T_{2}v_{m+1} &= 0 \\
&\cdots \\
T_{2}v_{v} &= 0 \\
\end{split}
\end{equation*}

so the target map $S$ would be:
\begin{equation*}
    \begin{split}
Su_{1} &= v_{1} \\
&\cdots \\
Su_{n} &= v_{n} \\
Su_{n+1} &= 0 \\
&\cdots \\
Su_{v} &= 0 
\end{split}
\end{equation*}

because 
\[T_{2}S(u_{1}) = T_{2}(v_{1}) = w_{1} = T_{1}u_{1} \] for all $u$.

\subsubsection*{3.B.27}

For a polynomial p with deg n, we write its vector as 
\[a_{1}, \cdots, a_{n}\]

and for polynomial q with deg $n+1$, we write its vector as
\[b_{1}, \cdots, b_{n+1} \]

then $q^{'}$ has deg n:
\[b_{2}, 2b_{3}, 3b_{4},\cdots, nb_{n+1}\]

and $q^{''}$ has deg $n-1$:

\[2b_{3}, 6b_{4}, 12b_{5},\cdots, n*(n-1)b_{n+1}\]

so $q$ can be defined as:

\begin{equation*}
    \begin{split}
    a_{n} &= 3nb_{n+1} \\
    a_{n-1} &= 3(n-1)b_{n} + 5n*(n-1)b_{n+1} \\
    &\cdots \\
    a_{1} & = 3b_{2} + 52b_{3}
\end{split}
\end{equation*}

we can calculate $b_{n+1},\cdots, b_{1}$ top-down.

\subsubsection*{3.B.28}

$w_{1}, \cdots, w_{m}$ is a basis of range $T$, so we have $v_{1},\cdots, v_{m}$ that $\{Tv_{i} = w_{i}, i\in [1, 2,\cdots, m]$. 
It's easy to prove that $v_{1}, \cdots, v_{m}$ is independent(if not, $w_{1}, \cdots, w_{m}$ will also be dependent). Now expand the vector to a basis of $V: v_{1},\cdots, v_{m},\cdots, v_{n}$.

$\forall v, \exists a_{1},\cdots, a_{m}\in F$ that:
\[v = a_{1}v_{1} + \cdots + a_{m}v_{m}\]

also $a_{1},\cdots,a_{m}$ is uniq for any $v\in V$.

Now define

\begin{equation*}
    \begin{split}
\varphi_{1} &= a_{1} \\
\varphi_{2} &= a_{2} \\
&\cdots \\
\varphi_{m} &= a_{m}
\end{split}
\end{equation*}

$a_{1}, \cdots, a_{m}$ is the scala in the equation.

\subsubsection*{3.B.29}

dim $F = 1$, so a basis of $V$ is like
\[v_{1},\cdots,v_{n}, v_{n+1}\]

and $v_{1},\cdots, v_{n}$ is the basis of $\varphi$. 

Now consider the span of $v_{n+1} = U$, it's easy see $U \cap $ null $\varphi = \{0\}$ and $U + $ null $\varphi = V$. because

\[U = \{av_{n+1}: a\in F\} = \{au: a\in F\]
so $V = $null $\varphi \oplus \{au:a \in F\}$.

\subsubsection*{3.B.30}
Use the proof of 3.B.29, if $v_{n+1}\in $basis of $V,\varphi_{1}v_{n+1} = a_{1} \neq 0, \varphi_{2}v_{n+1} = a_{2} \neq 0$,  So 
\begin{equation*}
    \begin{split}
\varphi_{1}v_{n+1} &= a_{1} \\
\varphi_{2}v_{n+1} &= a_{2} \\
\frac{\varphi_{1}}{\varphi_{2}} & = \frac{a_{1}}{a_{2}} \\
\varphi_{1} &= c\varphi_{2}
\end{split}
\end{equation*}

\subsubsection*{3.B.31}
\[T_{1}(a_{1}, \cdots, a_{5}) = (a_{4}, a_{5})\]
\[T_{2}(a_{1}, \cdots, a_{5}) = (a_{5}, a_{4})\]

\subsection*{3.C}
\subsubsection*{3.C.1}

If the less than range $T$ nonzero entries, then in the matrix defined in 3.30, at least one column $k$ will be all zero, then in 3.32, the $Tv_{k}$ would be zero, so there would be less than dim range $T$ nonzero vectors in $TV$: they can't span a space with dim range $T$.

\subsubsection*{3.C.2}

basis of $P_{3} = (0, 0, 0, 1), (0, 0, 1, 0), (0, 1, 0, 0), (1, 0, 0,0)$ 

basis of $P_{2} = (3, 0, 0), (0, 2,  0), (0, 0, 1)$

\subsubsection*{3.C.3}

Suppose $u_{1}, \cdots, u_{n}$ is a basis of null $T$, then we can extend the vector to a basis of $V$: $v_{1},\cdots, v_{m}, u_{1}, \cdots, u_{n} $, also we have dim range $T = m$, and $Tv_{1}, \cdots, Tv_{m}$ is a basis of range $T$.

so here is the $m \times (m+n)$ matrix  we need:

\[
\begin{pmatrix}
1 & 0 & \cdots & 0 & \cdots & 0\\
0 & 1 & \cdots & 0 & \cdots & 0\\
\cdots \\
\cdots &  & & 1 & \cdots & 0 
\end{pmatrix}
\]

\subsubsection*{3.C.4}

If $Tv_{1} = 0$ , then entries of the 1st column are all 0; If no, $w_{1} = Tv_{1}$, and extend $w_{1}$ to a basis of $W$, then the entries of 1st column have a initial 1 followed by 0.

\subsubsection*{3.C.5}

If $w_{1} \notin $ range $T$, then every entity in the 1st row of $M$ has to be 0; 

If $w_{1} \in $ range $T$, then $\exists v_{1} \in V, Tv_{1} = w_{1}$, then any basis with $v_{1}$ as the 1st element make the 1st row has a initial 1 followed by 0.

\subsubsection*{3.C.6}

Suppose $v_{1}, \cdots, v_{n}$ is a basis of $V$ and $w_{1} = Tv_{1} \neq 0$, it's easy to know that $v_{1}, v_{2} + v_{1}, \cdots, v_{n}+v_{1}$ is also a basis. Because dim range $T = 1$, so we have

\begin{equation*}
    \begin{split}
    Tv_{1} & = w_{1} \\
    T(v_{2} + v_{1}) & = a_{2}w_{1} \\
    & \cdots \\
    T(v_{n} + v_{1}) & = a_{n}w_{1}
\end{split}
\end{equation*}

So the basis of $V$ is $v_{1},  \frac{v_{2} + v_{1}}{a_{2}}, \cdots, \frac{v_{n}+v_{1}}{a_{n}}$ and the basis of $W$ is $w_{1}$.


If we have a $M$ with all entries equals to 1, then there is a basis of $V$ and $W$ that $Tv = w, v\in$ basis $V$, then it's easy to know every $v\in V$ is a linear combination of basis $V$, so $Tv$ is linear combination of $w$, so $w$ is a basis of range $T$, which means than dim range $T = 1$.

\subsubsection*{3.C.10}

In $3.47$ we learn that
\[(AC)_{j,k} = A_{j,.}C_{.,k}\]

so 

\begin{equation*}
    \begin{split}
    (AC)_{j,.} &= [(AC)_{j,1}, \cdots, (AC)_{j,k}] \\
    &= [A_{j,.}C_{.,1}, \cdots, A_{j,.}C_{,.k}] \\
    &= A_{j,.}[C_{.,1}, \cdots, C_{.,k}] \\
    &= A_{j,.}C
\end{split}
\end{equation*}

\subsubsection*{3.C.11}

\begin{equation*}
    \begin{split}
aC &= [\sum_{r=1}^{n}a_{r}C_{r,1}, \sum_{r=1}^{n}a_{r}C_{r,2},\cdots, \sum_{r=1}^{n}a_{r}C_{r,p}] \\
&= \sum_{r=1}^{n}[a_{r}C_{r,1}, a_{r}C_{r,2}, \cdots, a_{r}C_{r,4}] \\
&= \sum_{r=1}^{n}a_{r}C_{r,.} \\
&= a_{1}C_{1,.} +\cdots + a_{n}C_{n,.}
\end{split}
\end{equation*}

\subsubsection*{3.C.14}

In $3.47,3.49, 3.C.10$ we learn that

\begin{equation*}
    \begin{split}
(AC)_{j,k} &= A_{j,.}C_{.,k} \\
(AC)_{.,k} &= AC_{.,k} \\
(AC)_{k,.} &= A_{k,.}C
\end{split}
\end{equation*}
so 

\begin{equation*}
    \begin{split}
    A(BC)_{j,k} &= A_{j,.}(BC)_{.,k} \\
    & = A_{j,.}(BC_{.,k}) \\
    (AB)C_{j,k} &= (AB)_{j,.}C_{.,k} \\
     &= (A_{j,.}B)C_{.,k}
\end{split}
\end{equation*}

expand both $A(BC)_{j,k}$ and $(AB)C_{j,k}$, they all equals to (if $B$ is $m*n$ matrix):
\[\sum_{r=1}^{m}\sum_{s=1}^{n}A_{j,m}B_{m,n}C_{n,k}\]

so $A(BC) = (AB)C$

\subsection*{3.D}
\subsubsection*{3.D.1}

Because $S$ and $T$ are both invertible, then we have
\[ST\times T^{-1}S^{-1} = S(TT^{-1})S^{-1} = SIS^{-1} = I\]

and 
\[T^{-1}S^{-1} \times ST = T^{-1}(S^{-1}S)T = T^{-1}IT = I\]

so $ST$ is invertible and $(ST)^{-1} = T^{-1}S^{-1}$

\subsubsection*{3.D.2}

Suppose $v_{1}, \cdots, v_{n}$ is a basis of $V$, consider operators:
\[T_{1}v_{1} = 0, T_{1}v_{i} = v_{i} if i > 1\]

and 
\[T_{2}v_{1} = v_{1}, T_{1}v_{i} = 0 if i > 1\]

They are neither injective, so they are not invertible, and we have 
\[T_{1} + T_{2} = I\]
which is invertible, so the noninvertible operators is not a subspace.

\subsubsection*{3.D.3}
$U$ is a subspace of $V$, so for a basis of $U: v_{1}, \cdots, v_{u}$, we can extend it to a basis of $V: v_{1},\cdots, v_{u}, \cdots, v_{n}$.

If $T$ is invertible and $Tu = Su$, then null $T = {0}$, so $S$ is injective.

If $S$ is injective, then $\forall u \in U, Su \neq {0}$, so $Sv_{1}, \cdots, SV_{u}$ is linear independent. 
Expand the vector to a basis of $V: Sv_{1}, \cdots, Sv_{u}, \cdots, v^{'}_{n}$, and make
\[Tv_{u+1} = v^{'}_{n+1}, \cdots, Tv_{n} = v^{'}_{n}\]

then it's easy to know that T is both injective and surjective, so it's invertible.

\subsubsection*{3.D.4}

If $\exists S, T_{1} = ST_{2}$, then $\forall v\in V, T_{1}v = 0 $ we have
\[T_{1}v = ST_{2}v = 0\]

Since null $T = {0}$, then we have $T_{2}v = 0$ so null $T_{1} = $ null $T_{2}$.

If null $T_{1} = $ null $T_{2}$, then we have $(v_{1},\cdots, v_{n})\, T_{1}v_{i} \neq 0, T_{2}v_{i} \neq 0$.
So we can define $S$ as 
\[S(T_{2}v) = T_{1}v\, v\in (v_{1}, \cdots, v_{n})\]

because $S$ is both injective and surjective.

\subsubsection*{3.D.5}

If $\exists S$, $T_{1}v = (T_{2}S)v = T_{2}(Sv) = $ range $T_{2} v\in V$, so range $T_{1} = $ range $T_{2}$.

If range $T_{1} = $ range $T_{2}$, then dim null $T_{1} = $ dim null $T_{2}$, so we have 2 basis that
\[(v^{'}_{1}, \cdots, v^{'}_{m}, w^{'}_{1}, \cdots, w^{'}_{n})\, T_{1}v = 0, T_{1}w \neq 0\]
and
\[(v^{''}_{1}, \cdots, v^{''}_{m}, w^{''}_{1}, \cdots, w^{''}_{n})\, T_{2}v = 0, T_{2}w \neq 0\]

So we can get an invertible $S$ that
\[Sv^{''} = v^{'}, Sw^{''} = w^{'}\]

\subsubsection*{3.D.6}

If $\exists R, \exists S, T_{1} = ST_{2}R$, then $\forall v\in V\, T_{1}v = (ST_{2}R)v $, so null $T_{1} = $ null $ST_{2}R$.
Because $S,R$ are injective, so dim null $ST_{2}R$ = dim null $T_{2}R = $ dim null $T_{2}$.

If dim null $T_{1} = $ dim null $T_{2}$, then we have 2 basis
\[(v^{'}_{1}, \cdots, v^{'}_{m}, w^{'}_{1}, \cdots, w^{'}_{n})\, T_{1}v = 0, T_{1}w \neq 0\]
and
\[(v^{''}_{1}, \cdots, v^{''}_{m}, w^{''}_{1}, \cdots, w^{''}_{n})\, T_{2}v = 0, T_{2}w \neq 0\]

we make $R$ as $Rv^{'} = Rv^{''}, Rw^{'} = Rv^{''}$ as $S$ as $S(T_{2}v) = T_{1}v$, then we have $T_{1} = ST_{2}R$


\subsubsection*{3.D.7}
\begin{enumerate}[label=(\alph*)]
\item $E$ is subspace
If $T_{1} \in E, T_{2} \in E$, then $(T_{1} + T_{2}v = T_{1}v + T_{2}v = 0 + 0 = 0$. $(\lambda T)v = \lambda (Tv) = \lambda \times 0 = 0$, so $E$ is a subspace.
\item dim $E$
If $v\neq 0$, then we can expand it to a basis of $V: v, v_{2},\cdots, v_{n}$, so $E$ is actually $L($span$(v_{2}, \cdots, v_{n}), W)$, so 
dim $E = ($ dim $V) - 1) \times $ dim $W$.
\end{enumerate}

\subsubsection*{3.D.8}

$V \to W$ is surjective, so suppose null $T = (v_{1}, \cdots, v_{n})$, then we can extend null $T$ to a basis of $V: (v_{1}, \cdots, v_{n}, w_{1},\cdots, w_{m}$.
It's easy to know that $T_{w} \neq 0$, and it's a basis of W.

No define $U = $ span $(w_{1}, \cdots, w_{m}$, and $T|_{U}$ is both injective and surjective, so $T|_{U}$ is isomorphism of $U$ onto $W$.

\subsubsection*{3.D.9}

If $S,T$ are invertible, so 

\[ST\times T^{-1}S^{-1} = S(TT^{-1})S^{-1} = SS^{-1} = I \]

and

\[T^{-1}S^{-1}\times ST = T^{-1}(S^{-1}S)T = T^{-1}T = I \]

so $ST$ is surjective.

IF $ST$ is invertible, then $\forall v, \exists R$ that $R(STv) = I$. If $S$ or $T$ is not inevtible, then $\exists v $ that $STv = 0$, then $Rv = 0$ which is a counter case 
of $R(STv) = I$, so both $S$ and $T$ are invertible.

\subsubsection*{3.D.10}

If $ST = I$, so dim $S = $ dim $V$, so $S$ is surjective: if not so, there would be some $v$ not in range $S$, so $STv \neq v$. $V$ is fixed dimensional, so $S$ is invertible because $3.69$.

\[S^{-1}ST = S^{-1}I = S^{-1} = T\]

so $TS = I$.

\subsubsection*{3.D.11}

Because $3.D.10$, we have $S^{-1} = TU$ and $U^{-1} = ST$.

\begin{equation*}
    \begin{split}
    STU &= I \\
    S^{-1}STU &= S^{-1} \\
    TU &= S^{-1} \\
    TUS &= I
\end{split}
\end{equation*}
and 

\begin{equation*}
    \begin{split}
    STU &= I \\
    USTU &= U \\
    USTUU^{-1} &= UU^{-1} \\
    UST &= I
\end{split}
\end{equation*}
so we have $T^{-1} = US$

\subsubsection*{3.D.13}

Since $V$ is finite dimensional, and $RST$ is surjective, then $RST$ is invertible (3.69); In $3.D.9$, we know that $ST$ is invertable, then $S$ is invertible, so $S$ is injective.

\subsubsection*{3.D.14}

Injective: $\forall v \in V, v \neq 0, \exists a_{1}, \cdots, a_{n}$ that $Tv = [a_{1}, \cdots, a_{n}] \neq {0}$, so $T$ is injective, so $T$ is invertible because $3.69$.

\subsubsection*{3.D.15}

In $3.64$, we have $M(T)_{.,k} = M(v_{k})$, Consider a basis of $V: (v_{1} = [1, 0, \cdots, 0], \cdots, v_{n} = [0, \cdots, 1])$ and a matrix $M$:
\[
\begin{pmatrix}
Tv_{1}, \cdots, Tv_{n}
\end{pmatrix}
\]

for any $T$, It's easy to see that $Mv = Tv$.

\subsubsection*{3.D.20}

The first one equals that

\begin{equation}
    \begin{vmatrix}
    M
    \end{vmatrix} . 
    \begin{bmatrix}
    x_{1} \\ \cdots \\ x_{n}
    \end{bmatrix}  = 0
\end{equation}

If $x_{1} = \cdots = x_{n} = 0$, means $M$ is injective.

The 2nd one equals that

\begin{equation}
    \begin{vmatrix}
    M
    \end{vmatrix} . 
    \begin{bmatrix}
    x_{1} \\ \cdots \\ x_{n}
    \end{bmatrix} = 
    \begin{bmatrix}
    c_{1} \\ \cdots \\ c_{n}
    \end{bmatrix}
\end{equation}

exists a solution, which means that $M$ is invertible, 
they are equivalent because $3.69$.

\subsection*{3.E}
\subsubsection*{3.E.1}

If $T$ is linear, $\forall v_{1}, v_{2} \in T$, we have $T(v_{1} + v_{2}) = T(v_{1}+v_{2})$, which implies 
\[(v_{1}, Tv_{1}) + (v_{2}, Tv_{2}) = (v_{1}+v_{2}, Tv_{1} + Tv_{2}) = (v_{1} + v_{2}, T(v_{1} + v_{2}))\]

using $3.71$. So graph of $T$ is close under addition.

This process also works under scalar multiplication:

\[\lambda Tv = T(\lambda v)\]

so 

\[\lambda (v, Tv) = (\lambda v, \lambda Tv) = (\lambda v, T(\lambda v))\]

so graph of $T$ is linear.

If graph of $T$ is linear, just reverse the process above, we will see that T is linear.

\subsubsection*{3.E.2}

In $3.76$, we have 

\[dim(V_{1} \times \cdots \times V_{m} = dim V_{1} + \cdots + dim V_{m}\] 

so if $V_{1} \times \cdots \times V_{m}$ is finite-dimensional, then each vector space should be finite-dimensional.

\subsubsection*{3.E.3}

Make $U_{1} = P(R), $ and $U_{2} = P(R)$, then 

\[U_{1} \times U_{2} = a, a^{'}, bx, b^{'}x + \cdots \]

and 

\[U_{1} + U_{2} = P(R) = P(R)\]

they are isomorphic because $T$ is invertible if we define $T$ as:
\[T(U_{1} \times U_{2}) = T(a, a^{'}, bx, b^{'}x + \cdots) = a + a^{'}x + bx^{2} + x^{'}x^{3} + \cdots\]

and apparently $U_{1}+U_{2}$ are not direct sum.

\subsubsection*{3.E.4}

$L(V_{1}\times \cdots \times V_{m}, W)$ are maps from $(v_{1}, \cdots, v_{m}) \implies w, w\in W$. Now make $v_{1}, \cdots, v_{m}$ are the only non-zero item in the vector, then the map $S_{0}, \cdots, S_{m}$ would be the same as $T = L(V_{i}, W)$, which means than $\forall T\in L(V, w)$, there is a map equals to it.

For all $S \in L(V_{1}\times \cdots \times V_{m}, W)$, we can split use the previously step, so define
\[R(S) = R(S_{0} + \cdots + S_{m}) = (T_{0} , \cdots , T_{m}) \]

so 

\[S\implies T\] is injective.

Now consider the other direction, 
\[R(T_{0} , \cdots , T_{m}) = S_{0} + \cdots + S_{m} = S\]

$R$ is also injective. Singe the map are injective both direction, so it's a invertible.

\subsubsection*{3.E.5}

This exercise works like $3.E.4$, make $w_{1}, \cdots, w_{m}$ to a vector with only 1 non-zero tiem, then the map equals to $L(V, W_{i})$. so we can find 2 injective mapping with reverse direction.

\subsubsection*{3.E.7}

If $u = 0$, which also belongs to $U$, so $\exists w_{1}$ that 
\[v + 0 = x + w_{1}\]

and for any non-zero $u \in U, \exists w_{2} \in W$ that
\[v + u = x + w_{2}\] 

now use the 2nd equation minus the 1st equation, we got:
\[u = w_{2} - w_{1}\]

Since $W$ is subspace, so $\forall u \in W$, so $U = W$.

\subsubsection*{3.E.8}

If $A$ is a affine subset, then $\exists u, U: u\in V, A = u + U$. $\forall v, w \in A, \lambda \in F, \exists v^{'}, w^{'} \in U$, that

\begin{equation*}
    \begin{split}
    v &= u + v^{'} : v^{'} \in U \\
    w &= u + w^{'} : w^{'} \in U \\
    \lambda v + (1-\lambda)w &= \lambda(u + v^{1}) + (1-\lambda)(u+w^{'}) \\
    &= \lambda u + \lambda v^{'} + (1-\lambda)u + (1-\lambda)w^{'} \\
    &= u + \lambda v^{'} + (1-\lambda)w^{'}
\end{split}
\end{equation*}

Since $U$ is subspace,so $(\lambda v^{'} + (1-\lambda)w^{'}) \in U$, then $(u+\lambda v^{'} + (1-\lambda)w^{'}) \in A$.

To prove that $A$ is affine subset, define $U = {A - v: v\in A, v\neq 0}$, we need to prove that $U$ is a vector space.

$\forall a\in A, (a-v)\in U$, so $\forall \lambda \in F$,

\begin{equation*}
    \begin{split}
    \lambda (a-v) &= \lambda a - \lambda v \\
    &= \lambda a + v - \lambda v -v \\
    & = \lambda a + (1-\lambda)v -v \\
    &= (\lambda a + (1-\lambda)v) - v \in {A-v} = U
\end{split}
\end{equation*}
so $U$ is close under scalar multiplication.

$\forall a, b \in A$, $a-v \in U, b-v\in U$, so $(a-v)/2 \in U, (b-v)/2 \in U$ (by the previously steps).
\begin{equation*}
    \begin{split}
    \frac{a-v}{2} + \frac{b-v}{2} &= \frac{a}{2} + \frac{b}{2} - v \\
    &= \frac{1}{2}a + (1-\frac{1}{2})b -v \\
    &= (\lambda a + (1-\lambda)b) -v \in {A-v} = U
    \end{split}
\end{equation*}

so $U$ is close under addition.

\subsubsection*{3.E.9}

If $A_{1} \cap A_{2} $ is not empty, then there would be at least 1 vector $v$ in both affine subsets, then we have
\[A_{1} = v + U_{1}, A_{2} = v + U_{2}\] 
because $3.85$. Now let's talk about $U_{1}, U_{2}$: 

If $U_{1} \cap U_{2} = {0}$, then $v + {0}$ is a affine subset.

If $U_{1} \cap U_{2} = U_{3} \neq {0}$, then it's easy to know that $U_{3}$ is also a subspace, then

\begin{equation*}
    \begin{split}
    A_{1} \cap A_{2} &= {v + U_{1}} \cap {v + U_{2}} \\
    &= {v + U_{1}\cap U_{2}} \\
    &= {v + U_{3}}
        \end{split}
\end{equation*}

so $A_{1} \cap A_{2} $ is a affine subset or empty.

\subsubsection*{3.E.10}

Use the solution of $3.E.9$, merge 2 affine subsets into a new affine subsets, or a blank set. 

\subsubsection*{3.E.11}

\begin{enumerate}[label=(\alph*)]
\item $A$ is affine subset

like $3.E.8$, we prove that $A-v: v\in A$ is a subspace. It's easy to know that $v_{1} \in A$ because $\lambda_{1} = 1$ will prove that.

\begin{equation*}
    \begin{split}
    k (A-v_{1}) &= k(\lambda_{1} - 1)v_{1} + k\lambda_{2}v_{2} + \cdots + k\lambda_{m}v_{m} \\
    &= k(\lambda_{1} - 1)v_{1} + k\lambda_{2}v_{2} + \cdots + k\lambda_{m}v_{m} + v_{1} - v_{1} \\
    &= (k\lambda_{1} - k + 1)v_{1} + k\lambda_{2}v_{2} + \cdots + k\lambda_{m}v_{m}  - v_{1}
    \end{split}
\end{equation*}

$(k\lambda_{1} - k + 1) + k\lambda_{2} + \cdots + k\lambda_{m} = k\sigma_{1}^{m}\lambda - k + 1 = 1$, 
so $\forall v \in (A-v_{1}), kv \in (A-v_{1})$, which means $A-v_{1}$ is close under multiplication.

The same trick works on addition

\begin{equation*}
    \begin{split}
    (\lambda_{11} - 1)v_{1} + \lambda_{21}v_{2} + \cdots + \lambda_{m1}v_{m} &+ \\
    (\lambda_{12} - 1)v_{1} + \lambda_{22}v_{2} + \cdots + \lambda_{m2}v_{m} &= (\lambda_{11} - 1)v_{1} + \lambda_{21}v_{2} + \cdots + \lambda_{m1}v_{m} +\\
    & (\lambda_{12} - 1)v_{1} + \lambda_{22}v_{2} + \cdots + \lambda_{m2}v_{m} + v_{1} - v_{1} \\
    &= (\lambda_{11} + \lambda_{21} - 1)v_{1} + (\lambda_{21}+\lambda_{22})v_{2} + \cdots + (\lambda_{m1}+\lambda_{m2})v_{m} - v_{1} \\
    \sigma_{1}^{m}\lambda_{1} + \sigma_{1}^{m}\lambda_{2} - 1 &= 1 + 1 - 1 \\
    &= 1
    \end{split}
\end{equation*}

so $A-v_{1}$ is close under addition, Which means $A$ is affine subset.

\item $W\supset A$

Since $v_{1}\in B$ (B is a affine subset of $V$ than $v_{1},\cdots,v_{m} \in B$, then we have $B = v_{1} + W$, $W$ is a subspace of $V$(because 3.85). also we have $v_{2}-v_{1}, \cdots, v_{m}-v_{1} \in W$.

So it's easy to know that $\forall \lambda_{2},\cdots, \lambda_{m} \in F$

\begin{equation*}
    \begin{split}
\lambda_{2}(v_{2}-v_{1}) + \cdots + \lambda_{m}(v_{m}-v_{1}) &\in W \\
v_{1} + \lambda_{2}(v_{2}-v_{1}) + \cdots + \lambda_{m}(v_{m}-v_{1}) &\in B \\
v_{1} + \lambda_{2}(v_{2}-v_{1}) + \cdots + \lambda_{m}(v_{m}-v_{1}) &= (1-\lambda_{2} - \cdots - \lambda_{m})v_{1} + \lambda_{2}v_{2} + \cdots + \lambda_{m}v_{m} \\
(1-\lambda_{2} - \cdots - \lambda_{m}) &= \lambda_{1} \\
\lambda_{1} + \cdots + \lambda_{m} &= 1 \\
v_{1} + \lambda_{2}(v_{2}-v_{1}) + \cdots + \lambda_{m}(v_{m}-v_{1}) &\in A
    \end{split}
\end{equation*}

\item dim $U\leq m-1$
Consider the vector $(v_{1} - v_{1}, v_{2} - v_{1}, \cdots, v_{m} - v_{1})$, there is a $0$ and less than or equal to $m-1$ non-zero items. Based on 3.85, span$(v_{2}-v_{1}, \cdots, v_{m}-v_{1}) + v = A$. Based on $2.31$, dimension of span$(v_{2}-v_{1}, \cdots, v_{m}-v_{1}) \leq m-1$.
\end{enumerate}

\subsubsection*{3.E.12}

Because $U$ is a subspace of $V$, so there exists a subspace $W$ of $V$ that $V = U \oplus W$. $\forall v\in V, v = w + u: w\in W, u\in U$. It's easy to know that $W$ is isomorphic to $V/U$ because $v+U = w + U$.

now we add $U$ into the equation, $U\times W$ is isomorphic to $U \times V/U$, which equals to that $V$ is isomorphic to $U\times V/U$.

\subsubsection*{3.E.13}

Firstly, $v_{1},\cdots,v_{m}, u_{1},\cdots,u_{n}$ is linear independent, because if it's not, then we will get a counter example that $v_{1}+U,\cdots, v_{m}+U$ is a basis of $V/U$.

If $v_{1},\cdots,v_{m}, u_{1},\cdots,u_{n}$ cant span $V$, then there exists a $w\in V$ that $v_{1},\cdots,v_{m}, u_{1},\cdots,u_{n}, w$ is linear independent, then $v_{1}+U,\cdots, v_{m}+U, w+U$ is linear independent, which is also a counter example of $v_{1}+U,\cdots, v_{m}+U$ is a basis of $V/U$.

\subsubsection*{3.E.14}

\begin{enumerate}
    \item subspace
    
    If $u\in U$, then $\lambda u = {(\lambda x_{1}, \lambda x_{2},\cdots )}$, it's easy to see that there are finitely non zero elements, because $\lambda 0 = 0$. for $u_{1}\in U, u_{2} \in U$, $u_{1} + u_{2}$ is also has finitely non zero elements because the max number of non zero elements is the sum of count of non zero elements in the 2 vectors.

    \item infinite-dimensional
    
    Make 
    \[v_{j} = (x_{1},x_{2},\cdots) \in F^{\infty}, x_{i} = 0\, if\, i < j, x_{i} = 1\, if\, i \geq j\]
    
    and
    
    \[W = {v_{i} i\in F}\]
    
    $W$ is a subspace of $F^{\infty}$ and it's infinite dimensional, also $W\cap U = {0}$, so $W/U$ is infinite dimensional, so $F^{\infty}/U$ is infinite-dimensional.
\end{enumerate}

\subsubsection*{3.E.15}

$\phi \in L(V, F)$, so dim null($\phi$) = dim $V$ - dim $F$ = dim $V$ - 1, so dim $V/(null \phi)$ = dim $V$ - (dim $V$ - 1) = 1.

\subsubsection*{3.E.16}

dim $V/U = 1$, so dim $U = $ dim $V - 1$ (3.89), so for a basis of $U: (u_{1}, \cdots, u_{n})$, we can expand it to a basis of $V: (u_{1}, \cdots, u_{n}, v)$. 

$\forall v = a_{1}u_{1}+\cdots a_{n}u_{n} + bv$, define 
\[\phi(v) = b\]

$\phi \in L(V, F)$ and it's easy to check  null $\phi = U$.

\subsubsection*{3.E.16}

Use the proof of $3.E.13$, the basis of $W$ is $v_{1}, \cdots, v_{n}$.

\subsection*{3.F}
\subsubsection*{3.F.1}

If $\varphi$ is a linear function, and $\varphi(V) = {0} $, then it's a zero map. If not, then dim(range($\varphi$)) = 1, and it's surjective because dim(F) = 1.

\subsubsection*{3.F.3}

$\exists \varphi^{'} \in V^{'}$ that $\varphi^{'}(v) \neq 0$. then make $\varphi = (\varphi^{'}(v))^{-1}$.

\subsubsection*{3.F.4}

Suppose $u_{1}, \cdots, u_{m}$ is a basis of $U$ and $u_{1}, \cdots, u_{m}, v_{1},\cdots, v_{n}$ is a basis of $V$ because $V\neq U$. then use $3.96$, $\varphi_{m+1},\cdots,\varphi_{m+n}$ are the target linear functions we're looking for.

\subsubsection*{3.F.5}

They're isomorphic because they have the same dimension.

based on $3.95, 3.76$, dim $(V_{1}\times \cdots \times V_{m})^{'} = $ dim $(V_{1}\times \cdots \times V_{m})$ = dim $V_{1} + \cdots + $dim $V_{m}$

and dim $V_{1}^{'}\times \cdots \times V_{m}^{'}$ = dim $V_{1}^{1}+\cdots +$ dim $V_{m}^{'}$ = dim $V_{1} + \cdots + $dim $V_{m}$.

\subsubsection*{3.F.6}

(a)

If $v_{1},\cdots,v_{m}$ can't span $V$, then $\exists v\in V, \notin$ span $(v_{1}, \cdots, v_{m})$. Now we can find a function $\varphi$ using $3.96$, then $\varphi \in $ null $\Gamma \neq {0}$, Which in a counter case.


When $\Gamma$ is injective, then null $\Gamma = {0}$. If $v_{1},\cdots,v_{m}$ can't spans $V$, then we can find a function $\varphi \in V^{'}$  and $G(\varphi) = 0$ using the same step, which is a counter case of null $\Gamma = 0$.

(b) 

If $v_{1},\cdots,v_{m}$ is linearly independent, make $W = $span($v_{1},\cdots,v_{m}$), then dim $W$ = m. According to $3.106$, dim $V$ = dim $W$ + dim $W^{0}$.

Now consider $\Gamma$ as a transaction, dim $V^{'} = $ dim null $\Gamma$ + dim range $\Gamma$. 

$\forall w^{0} \in W^{0}, \Gamma(w^{0}) = 0$, so dim null $\Gamma = $ dim $W^{0}$. so range $\Gamma = $ range $W = m$. Then it's surjective because dim $\Gamma = m = $ dim $(F^{m})$.

The calculation works on counter direction, so dim $W = m$, then $v_{1}, \cdots, v_{m}$ are linearly independent.

\subsubsection*{3.F.7}

for $p_{m} = x^{m}$, it's easy to know that
\[
\varphi_{j}(p_{m}) =
\begin{cases}
p^{m-j}(0) = 0,&j<m \\
1,&j=m \\
0,&j>m \\
\end{cases}
\]

which matches the define of $3.96$.

\subsubsection*{3.F.8}
(a)

The degrees of each element are $(0,1,2,\cdots, m)$, so they are linearly independent.

(b)

Let's refer $3.F.7$, it's dual basis is
\[
\varphi(j)(p) = \frac{p^{j}(5)}{j!}
\]


\subsubsection*{3.F.9}

Since $\varphi_{1},\cdots,\varphi{n}$ is the dual basis, then we have
\[
\psi = a_{1}\varphi_{1} + \cdots + a_{n}\varphi_{n} 
\]

Based on $3.96$, $\psi(v_{i}) = 0 + \cdots + a_{i}\times 1+\cdots+0$, so $a_{i} = \psi(v_{i})$

Then we have
\[
\psi = \psi(v_{1})\varphi_{1} + \cdots + \psi(v_{n})\varphi_{n}
\]

\subsubsection*{3.F.11}

If $\exists (\VEC{c}{m}), (\VEC{d}{n})$ and $A_{i,k} = c_{j}d_{k}$, then we can write $A$ as:

\[
A = 
\begin{bmatrix}
\VEC{c_{1}d}{n} \\
\VEC{c_{2}d}{n} \\
\cdots \\
\VEC{c_{m}d}{n} 
\end{bmatrix}
\]

Then we have $A_{1,.} = \frac{A_{i,.} \times c_{1}}{c_{i}} i = 1,\cdots,n$. so the rank of $A$ is 1.

If rank of $A$ is 1, then make $c_{1} = 1, d_{1} = A_{1, 1}$. then $\VEC{d}{n} = A_{1,.}$ 
and $\VEC{c}{m} = \frac{A_{.,1}}{A_{1,1}}$.

\subsubsection*{3.F.12}
By $3.99$, we have 
\[
\varphi(v) = (T^{'}(\varphi))(T(v)) , v\in V, \varphi \in V^{'}
\]

Since $T(v) = Iv = v$, then we have $\varphi(v) = (T^{'}(\varphi))(v)), v\in V$, which means 
$\varphi = T^{'}(\varphi) \implies T^{'}(\varphi) = I\varphi$.

\subsubsection*{3.F.13}

\begin{enumerate}[label=(\alph*)]
\item 
\[
T^{'}(\varphi_{1}) = 4x + 5y + 6z \\
T^{'}(\varphi_{2}) = 7x + 8y + 9z
\]

\[ \varphi_{1}(T(v)) = T^{'}(\varphi_{1})(v), v\in R^{3} \]
\item

\begin{equation*}
M(T) = 
\begin{bmatrix}
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix}
\end{equation*}
\begin{equation*}
M(T^{'}) = (M(T))^{t} = 
\begin{bmatrix}
4 & 7 \\
5 & 8 \\
6 & 9
\end{bmatrix}
\end{equation*}
\[
T^{'}(\varphi_{1}) = 4\psi_{1} + 5\psi_{2} + 6\psi_{3}
\]
\[
T^{'}(\varphi_{2}) = 7\psi_{1} + 8\psi_{2} + 9\psi_{3}
\]

\end{enumerate}

\subsubsection*{3.F.14}
\begin{enumerate}[label=(\alph*)]
\item 
\begin{equation*}
    \begin{split}
T^{'}(\varphi)(x) &= (x^{2}p(x) + p^{''}(x))^{'}(4) \\
&= 8p(4) + 16p^{'}(4) + p^{'''}(4)
\end{split}
\end{equation*}

\item
\begin{equation*}
    \begin{split}
(T^{'}(\varphi))(x^{3}) &= (\varphi \circ T)(x^{3}) \\
& = \varphi (T(x^{3})) \\
& = \varphi(x^{5} + 6x) \\
& = 3 + \frac{1}{6}
\end{split}
\end{equation*}
\end{enumerate}

\subsubsection*{3.F.15}
Because $3.107.b$, dim null $T^{'} = $ dim null $T +$ dim $W$ - dim $V$. Suppose dim $W = w$, dim $V = v$, then we have
dim null $T^{'} = $ dim null $T + w - v$.

If dim null $T^{'} = w$, then we have dim null $T =v$, which indicates $T = 0$; and if $T = 0$, we have dim null $T^{'} = w$ which means $T^{1} = 0$.

\subsubsection*{3.F.16}
Because $3.114.a, M(T^{'}) = (M(T))^{t}$, then for $\forall T\in L(V, W)$, there would be a uniq $T^{'}$ whose matrix is the transpose of the matrix of $T$. It also works the opposite way, so it's a invertible map.

\subsubsection*{3.F.17}

If $U\subset null \varphi$, then $\forall u\in U, u\in null \varphi$, then $\varphi u = 0$, which is exactly the saying in $3.102$.
\subsubsection*{3.F.18}
By $3.95 $, dim $V^{'} = $ dim $V$ and $3.106 $, dim $U +$ dim $U^{0} = $ dim $V$, we have

dim $U +$ dim $U^{0} = $ dim $V^{'}$. 

IF $U = 0$, then dim $U^{0} =$ dim $V^{'}$, which means $U^{0} = V'{'}$, and If $U^{0} = V'{'}$ then $U = 0$.

\subsubsection*{3.F.19}
Leverage the same equation we use in $3.F.18$.

\subsubsection*{3.F.20}

If $W^{0}\not\subset U^{0}$, then $\exists \varphi_{j} \in W^{0}, \notin U^{0}$, $\varphi_{j}$ is the jth element of basis defined in $3.96$. Now consider the jth item of basis in $3.96 v_{j}, \varphi_{j}v_{j} = 1$, so $v_{j} \in U, \notin W$, which is conflict to $U\subset W$

\subsubsection*{3.F.21}

Suppose $\VEC{v}{n}$ is a basis of $V$, and $U\not\subset W$, then there must be a $v_{j} \in U, \notin W$. Now consider 
$\varphi_{j}$ defined in $3.96$, apparently $\varphi_{j} \in W^{0}, \notin U^{0}$, which is a counter case of $W^{0} \subset U^{0}$.

\subsubsection*{3.F.22}
Continue using $\varphi_{j}, v_{j}$ defined in $3.96$, the basis of $(U+W)^{0}$ is ${\varphi_{j}, v_{j} \notin U, \notin W}$.

The basis of $U^{0} \cap W^{0}$ is basis of $U^{0} \cap $ basis of $ W^{0}$, 
which is ${\varphi_{j}, v_{j} \notin U} \cat {\varphi_{j}, v_{j} \notin W}$, 
it's equal to ${\varphi_{j}, v_{j} \notin U, \notin W}$.

\subsubsection*{3.F.23}

$\varphi_{j}, v_{j}$ are items defined in $3.96$, then
\begin{equation*}
    \begin{split}
(U \cap W)^{0} &= span({\varphi_{i}, v_{i} \notin (U \cap W)}) \\
U^{0} &= span({\varphi_{i}, v_{i} \notin U}) \\
W^{0} &= span({\varphi_{i}, v_{i} \notin W}) \\
U^{0} + W^{0} &= span({\varphi_{i}, v_{i} \notin U}) \cup span({\varphi_{i}, v_{i} \notin W}) \\
&= span({\varphi_{i}, v_{i} \notin U} \cup {\varphi_{i}, v_{i} \notin W}) \\
&= span({\varphi_{i}, v_{i} \notin U \cup  v_{i} \notin W}) \\
&= span({\varphi_{i}, v_{i} \notin (U \cap W)}) \\
&= (U \cap W)^{0}
\end{split}
\end{equation*}

\subsubsection*{3.F.25}

$\forall u \in U, \varphi(u) = 0$ because that's the definition of annihilator.

If $v\in V, \notin U, \varphi(v) =0 \varphi \in U^{0}$, then $v = a_{1}v_{1} + \cdots + a_{n}v_{n}, \VEC{v}{n} $ is a basis of $V$, then by the definition of subspace, $\exists i , a_{i} \neq 0, v_{i} \notin U$. Also, because $\varphi(v) = 0$, so $\varphi_{i} \notin U^{0}$ ($\varphi_{i}$ is the dual basis defined in $3.96$. That is a conflict because $\forall u\in U, \varphi_{i}(u) = 0$ because $v_{i} \notin U$, so $v \in U$ if $\varphi(v) = 0 \forall \varphi \in U^{0}$.

\subsubsection*{3.F.26}

If $\Gamma$ is a subspace of $V^{'}$, then we have its basis $\VEC{\varphi}{n}$. Use $3.96$, define a subspace $U$ of $V$, whose basis is $v_{n+1},\cdots,v_{m}$, use $3.F.25$, it's easy to know that $U^{0} = \Gamma$.

\subsubsection*{3.F.27}

Because $3.107 (a)$: null $T^{'} = ($ range $T)^{0}$. and null $T^{'} = $ span $(\varphi), \varphi(p) = p(8)$, so $\forall p \in $ range $T, \varphi(p) = 0$, which equals to range $T = {p\in P_{i}(R): p(8) = 0}$.

\subsubsection*{3.F.28 & 3.F.29}

Use the method in $3.104$.

\subsubsection*{3.F.30}
Extend 
$\VEC{\varphi}{m}$ to $\VEC{\varphi}{n}$, 
a basis of $V^{'}$, and $\varphi_{i} = a_{i}\varphi^{'}_{1} + \cdots + n_{i}\varphi^{'}_{n}$, 
$\varphi^{'}$ is the dual basis in $3.96$. 
So it's easy to know $v_{i} = a_{i}v^{'}_{1} + \cdots + n_{i}v^{'}_{n}$ is a basis of $V$.

if $\forall v\in V, v = a_{1}v_{1} + \cdots + a_{n}v_{n}$, and make $T_{i} = a_{1}v_{1} + \cdots + 0v_{i} + \cdots + a_{n}v_{n}, i \in [1, m]$, then from $3.F.28$, null $\varphi_{i} = $ range $T_{i}, i\in [1, m]$, then

\begin{equation*}
    \begin{split}
    null \varphi_{1} \cap \cdots \cap null \varphi_{m} &= range T_{1} \cap \cdots \cap range T_{m} \\
    & = span(v_{1},\cdots, v_{n}) \cap \cdots \cap span(v_{1}, \cdots, v_{m-1}, v{m+1},\cdots, v_{n}) \\
    &= span(v_{m+1}, \cdots, v_{n})
\end{split}
\end{equation*}

so its dimension is dim $V - m$

\subsubsection*{3.F.31}

Since $\VEC{\varphi}{n}$ is a basis of $V^{'}$,  then their must be a mapping 
$T(\varphi_{i}) = e_{i}$, so  $M(T)\varphi_{i} = \varphi_{i}(e_{i})$, which we have $M_{i.,}\varphi_{j} = 1$ if and only if $i = j$, or it will be 0.

So the $M^{t}$ is the matrix we need, to convert $e$ to the basis we need.


\subsubsection*{3.F.33}
If $A, B\in F^{m,n}$, then $(A+B)^{t}_{j,k} = (A+B)_{k, j} = A_{k,j} + B_{k,j} = A^{t}_{j, k} +B^{t}_{j, k}$, so it's closure under sum.

$(kA)^{t}_{j, k} = (kA)_{k, j} = kA_{k,j} = kA^{t}_{j,}$, so it's closure under times.

Now let's prove it's invertible, and because $\forall A\in F^{m,n}$, there is an uniq $A^{t} \in F^{n,m}$, and vice versa.

\subsubsection*{3.F.34}
\begin{enumerate}[label=(\alph*)]
\item 
$(\Lambda v)(\varphi) = \varphi(v) \in F$, so $\Lambda v \in V^{''}$, then $\Lambda \in (V \implies V^{''}$.

\item
\begin{equation*}
    \begin{split}
    (T^{''}\omicron \lambda (v))(\varphi) &= (T^{''}(\lambda v))(p) \\
    &= (\lambda v \omicron T^{'})(\varphi) \\
    &= (\lambda v)(T^{'}(\varphi)) \\
    &= (T^{'}(\varphi))(v) \\
    &= (\varphi \omicron T)(v) \\
    &= \varphi(Tv) \\
    &= (\lambda \omicron T(v))(\varphi) \\
    &= ((\lambda \omicron T)(v))(\varphi)

\end{split}
\end{equation*}

\item
dim $V$ = dim $V^{'}$ = dim $V^{''}$, so $\Lambda$ is invertible mapping.
\end{enumerate}

\subsubsection*{3.F.35}

$T = {p(n), n\in R, p\in P(R)}$, so this is a 1-1 mapping.
\newpage